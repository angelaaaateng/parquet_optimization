{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap on Storage, ROW vs COLUMN\n",
    "\n",
    "<img src=\"img/1.png\"/>\n",
    "\n",
    "<img src=\"img/2.png\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what is parquet?\n",
    "\n",
    "<img src=\"img/3.png\"/>\n",
    "\n",
    "<img src=\"img/4.png\"/>\n",
    "\n",
    "<img src=\"img/5.png\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great, so about that lab...\n",
    "\n",
    "## How do I get into dumbo again?\n",
    "<img src=\"img/6.png\" width=\"400\"/>\n",
    "\n",
    "## Open terminal/bash/powershell... etc\n",
    "```sh\n",
    "ssh your_netid@gw.hpc.nyu.edu\n",
    "ssh dumbo.hpc.nyu.edu\n",
    "```\n",
    "\n",
    "## Onto the lab... what. How do I clone the repo? NotLikeThis\n",
    "\n",
    "1. Go to the [NYU-Big-Data](https://github.com/nyu-big-data) Git repo...\n",
    "2. Click on the Lab4-Storage link.\n",
    "3. Click the GREEN button named \"Clone\" to get your repo link:\n",
    "<img src=\"img/7.png\"/>\n",
    "4. Go back to your dumbo window...\n",
    "5. clone the repo:\n",
    "```sh\n",
    "git clone your_copied_repo_link\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Benchmarking queries\n",
    "\n",
    "If you can't write a query by now just quit the course. \n",
    "Seriously... save me the time and google the compute cycles.\n",
    "\n",
    "```sh\n",
    "+----------+---------+-------+-------+                                          \n",
    "|first_name|last_name| income|zipcode|\n",
    "+----------+---------+-------+-------+\n",
    "|   Annette|   Abbott|72870.0|  14763|\n",
    "|    Hailey|   Abbott|72182.0|  75097|\n",
    "|   Jocelyn|   Abbott|56574.0|   3062|\n",
    "|     Sheri|   Abbott|64952.0|  77663|\n",
    "|     Sonya|   Abbott|86156.0|  79072|\n",
    "+----------+---------+-------+-------+\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <strong>(Problem 1a)</strong> You need to write these queries:\n",
    "</div>\n",
    "\n",
    "1. csv_avg_income: the `average income` grouped by `zipcode`\n",
    "2. csv_max_income: the `maximum income` grouped by `last_name`\n",
    "3. csv_anna: people with `first_name` of 'Anna' and `income` at least 70000\n",
    "\n",
    "### I can write SQL... but where do I put it?\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>(Hint)</strong> Open queries.py\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm pseudo code example similar to what is found in queries.py\n",
    "#\n",
    "# --- ADD YOUR NEW QUERIES BELOW ---\n",
    "def fake_query(spark, file_path):\n",
    "    ## the query_name starts with CSV\n",
    "    df = spark.read.csv(file_path, header=True, \n",
    "               schema='first_name STRING, last_name STRING, income FLOAT, zipcode INT')\n",
    "    ## the query_name starts with PQ\n",
    "    df = spark.read.parquet(file_path)\n",
    "    df.createOrReplaceTempView('people')\n",
    "    \n",
    "    query = \"I'm the SQL query you meant to write, hear me roar!\"\n",
    "    fake_query = spark.sql(query)\n",
    "    return fake_query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "  Notice that, when reading a csv we need to either specify or infer a schema. This is because csv's are only semi-structured data. Just like relational DBs, parquet files are structured data - so spark is told expressly what to expect for each column when you read a parquet file.\n",
    "</div>\n",
    "\n",
    "### Awesome! But they want me to run 25 queries and save the output and stuff... \n",
    "\n",
    "I see you don't want to do any work. I've added a [csv_table helper function](https://github.com/sroy2/parquet_optimization/blob/master/bench.py#L94-L159) to my bench.py you can use.\n",
    "\n",
    "\n",
    "### Great, I copied the code - how do I call it?\n",
    "\n",
    "Aha! This is a good question!\n",
    "1. On dumbo go into the lab4 folder you cloned.\n",
    "2. load the following modules to tell dumbo what python and spark versions/environments we want to use:\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>(Hint)</strong> This is important if you want the right version of pyspark! I got lazy of typing them so I put them into my .bashrc (a config file that runs every time you log in).\n",
    "</div>\n",
    "\n",
    "```sh\n",
    "module load python/gnu/3.6.5\n",
    "module load spark/2.4.0\n",
    "pyspark\n",
    "```\n",
    "\n",
    "Now wait for pyspark to finish loading.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    You may wonder what all of those <strong>WARN Utils</strong> messages are about. The port messages are letting you know that the driver-node (dumbo in this case) can't bind or assign that port (<a href=\"https://spark.apache.org/docs/latest/monitoring.html\">starting at 4040</a>) for communication with the cluster manager. This is probably because someone else is running a spark instance that already has it bound.<br>\n",
    "</div>\n",
    "\n",
    "<img src=\"img/8.png\"/>\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    If you get <strong>WARN HiveConf</strong> (server misconfiguration) or <strong>WARN Client</strong> (you might be missing a declartion in your ~/.bashrc)... Well it probably doesn't matter. Ask on the <a href=\"https://groups.google.com/a/nyu.edu/forum/#!forum/ds-ga-1004-SP20-d65b\">Google Group</a> if your code doesn't run because of it. But... those warnings <strong>probably aren't</strong> why your code breaks.\n",
    "</div>\n",
    "\n",
    "```sh\n",
    "-bash-4.1$ more ~/.bashrc \n",
    "HADOOP_EXE='/usr/bin/hadoop'\n",
    "HADOOP_LIBPATH='/opt/cloudera/parcels/CDH/lib'\n",
    "HADOOP_STREAMING='hadoop-mapreduce/hadoop-streaming.jar'\n",
    "\n",
    "alias hfs=\"$HADOOP_EXE fs\"\n",
    "alias hjs=\"$HADOOP_EXE jar $HADOOP_LIBPATH/$HADOOP_STREAMING\"\n",
    "\n",
    "module load python/gnu/3.6.5\n",
    "module load spark/2.4.0\n",
    "-bash-4.1$ pyspark\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.6.5 (default, May  3 2018 09:34:46)\n",
    "SparkSession available as 'spark'.\n",
    "```\n",
    "\n",
    "### Great I'm in - now what?\n",
    "\n",
    "I'm just going to assume you wrote the queries above already. <br>\n",
    "\n",
    "Time to import our modules so we can call them from our spark session and read in the data!\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>(Hint)</strong> Don't forget to import modules now that you're in pyspark!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "import queries\n",
    "import bench\n",
    "\n",
    "queries.csv_anna(spark, 'hdfs:/user/bm106/pub/people_small.csv')\n",
    "_.show()\n",
    "\n",
    "bench.benchmark(spark, 25, queries.csv_anna, 'hdfs:/user/bm106/pub/people_small.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> _.show()\n",
    "+----------+---------+-------+                                                  \n",
    "|first_name|last_name| income|\n",
    "+----------+---------+-------+\n",
    "|      Anna|   Bailey|72458.0|\n",
    "|      Anna| Mcknight|74660.0|\n",
    "|      Anna|  Ferrell|74849.0|\n",
    "|      Anna|Velasquez|74693.0|\n",
    "+----------+---------+-------+\n",
    "\n",
    ">>> bench.benchmark(spark, 25, q.csv_anna, 'hdfs:/user/bm106/pub/people_small.csv')\n",
    "[0.0844266414642334, 0.08226633071899414, 0.08829736709594727, 0.0897524356842041, 0.07845854759216309, 0.07774877548217773, 0.07760429382324219, 0.0960390567779541, 0.0782325267791748, 0.07440423965454102, 0.07535243034362793, 0.08235287666320801, 0.07107186317443848, 0.07475018501281738, 0.07666397094726562, 0.07802414894104004, 0.07164120674133301, 0.07479310035705566, 0.0723581314086914, 0.08651256561279297, 0.07664608955383301, 0.07851982116699219, 0.07354259490966797, 0.07335448265075684, 0.08263015747070312]\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    Woaaah, what is that black-magic underscore thingy you did? In python an underscore is a placeholder variable that stores the output of the last statement you executed. In this case it's holding an unexecuted dataframe object. I execute that object by calling an action, in this case .show(), which triggers the spark operation.\n",
    "</div>\n",
    "\n",
    "### Awesome so your imported query works!\n",
    "\n",
    "*You said something about a helper function?*\n",
    "\n",
    "1. Ok you bum, I'm assuming you're writing your report in a jupyter notebook.\n",
    "2. We will use pandas for easy data import and table formatting (so import it!)\n",
    "3. You pass the entire line we just ran into the helper function as a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "test = \"bench.benchmark(spark, 25, queries.csv_anna, 'hdfs:/user/bm106/pub/people_small.csv')\"\n",
    "bench.csv_table(spark, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> bench.csv_table(spark, test)\n",
    "query,file,trials,minimum,median,maximum\n",
    "csv_anna,people_small,25,0.075,0.092,0.115\n",
    "```\n",
    "\n",
    "*Great! But... can't we run it over all the files?*\n",
    "\n",
    "1. Yes... I anticipated your needs.\n",
    "2. The helper function can take a third parameter to batch process tasks:\n",
    "\n",
    "```python\n",
    "    runtype : string | None, 'single', 'all', 'csv', 'pq'\n",
    "    \n",
    "        Optional parameter, if not set only benchmark_cmd results are printed.\n",
    "        \n",
    "        'all'    run all pq|csv queries on all file sizes\n",
    "        'csv'    runs csv queries with all bm106 csv filepath sizes\n",
    "        'pq'     runs pq queries with all provided pq filepath sizes\n",
    "        'both'   runs both, csv and pq queries as stated above\n",
    "```\n",
    "\n",
    "3. Let's try again, but this time with 'csv':\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <strong>(Warning)</strong> You are getting ready to loop over multiple files and/or queries multiple (hopefully less than 26) times! Jobs can go really slow. The progress bar is your friend. If you can't wait or think something broke, <strong>CTRL+C</strong> should break out. In the mean time get a coffee, watch a video, or take a nap.   : )\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "bench.csv_table(spark, test, 'csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> bench.csv_table(spark, test, 'csv')\n",
    "query,file,trials,minimum,median,maximum                                        \n",
    "csv_anna,people_small,25,0.066,0.075,0.164\n",
    "csv_anna,people_medium,25,0.562,0.579,0.731\n",
    "csv_anna,people_large,25,29.017,29.900,32.316\n",
    "```\n",
    "\n",
    "*Well look at that - but you didn't output to my jupyter notebook...*\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>(Hint)</strong> Just copy it!\n",
    "</div>\n",
    "\n",
    "<img src=\"img/9.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>file</th>\n",
       "      <th>trials</th>\n",
       "      <th>minimum</th>\n",
       "      <th>median</th>\n",
       "      <th>maximum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>csv_anna</td>\n",
       "      <td>people_small</td>\n",
       "      <td>25</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>csv_anna</td>\n",
       "      <td>people_medium</td>\n",
       "      <td>25</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>csv_anna</td>\n",
       "      <td>people_large</td>\n",
       "      <td>25</td>\n",
       "      <td>29.017</td>\n",
       "      <td>29.900</td>\n",
       "      <td>32.316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      query           file  trials  minimum  median  \\\n",
       "0  csv_anna   people_small      25    0.066   0.075   \n",
       "1  csv_anna  people_medium      25    0.562   0.579   \n",
       "2  csv_anna   people_large      25   29.017  29.900   \n",
       "\n",
       "   maximum                                          \n",
       "0                                            0.164  \n",
       "1                                            0.731  \n",
       "2                                           32.316  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_1 = pd.read_clipboard(sep=\",\")\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Great! But... why didn't you just run all the queries at once?*\n",
    "\n",
    "1. We could have, read the options for your helper function again.\n",
    "```python\n",
    ">>> bench.csv_table(spark, test, 'all csv')\n",
    "query,file,trials,minimum,median,maximum                                        \n",
    "csv_avg_income,people_small,25,1.299,1.790,2.282\n",
    "csv_avg_income,people_medium,25,2.050,2.281,2.892\n",
    "csv_avg_income,people_large,25,29.432,30.107,32.163\n",
    "csv_max_income,people_small,25,1.241,1.544,1.884\n",
    "csv_max_income,people_medium,25,1.871,2.135,5.779\n",
    "csv_max_income,people_large,25,30.263,37.220,42.168\n",
    "csv_anna,people_small,25,0.060,0.085,0.218\n",
    "csv_anna,people_medium,25,0.688,0.740,1.246\n",
    "csv_anna,people_large,25,34.957,39.114,43.670\n",
    "```\n",
    "\n",
    "2. Remember, the more things you chain together the longer it takes to process.\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <strong>(Problem 1b)</strong> You need to record your results:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "bench.csv_table(spark, test, 'all csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>file</th>\n",
       "      <th>trials</th>\n",
       "      <th>minimum</th>\n",
       "      <th>median</th>\n",
       "      <th>maximum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>csv_avg_income</td>\n",
       "      <td>people_small</td>\n",
       "      <td>25</td>\n",
       "      <td>1.299</td>\n",
       "      <td>1.790</td>\n",
       "      <td>2.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>csv_avg_income</td>\n",
       "      <td>people_medium</td>\n",
       "      <td>25</td>\n",
       "      <td>2.050</td>\n",
       "      <td>2.281</td>\n",
       "      <td>2.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>csv_avg_income</td>\n",
       "      <td>people_large</td>\n",
       "      <td>25</td>\n",
       "      <td>29.432</td>\n",
       "      <td>30.107</td>\n",
       "      <td>32.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>csv_max_income</td>\n",
       "      <td>people_small</td>\n",
       "      <td>25</td>\n",
       "      <td>1.241</td>\n",
       "      <td>1.544</td>\n",
       "      <td>1.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>csv_max_income</td>\n",
       "      <td>people_medium</td>\n",
       "      <td>25</td>\n",
       "      <td>1.871</td>\n",
       "      <td>2.135</td>\n",
       "      <td>5.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>csv_max_income</td>\n",
       "      <td>people_large</td>\n",
       "      <td>25</td>\n",
       "      <td>30.263</td>\n",
       "      <td>37.220</td>\n",
       "      <td>42.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>csv_anna</td>\n",
       "      <td>people_small</td>\n",
       "      <td>25</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>csv_anna</td>\n",
       "      <td>people_medium</td>\n",
       "      <td>25</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.740</td>\n",
       "      <td>1.246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>csv_anna</td>\n",
       "      <td>people_large</td>\n",
       "      <td>25</td>\n",
       "      <td>34.957</td>\n",
       "      <td>39.114</td>\n",
       "      <td>43.670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            query           file  trials  minimum  median  \\\n",
       "0  csv_avg_income   people_small      25    1.299   1.790   \n",
       "1  csv_avg_income  people_medium      25    2.050   2.281   \n",
       "2  csv_avg_income   people_large      25   29.432  30.107   \n",
       "3  csv_max_income   people_small      25    1.241   1.544   \n",
       "4  csv_max_income  people_medium      25    1.871   2.135   \n",
       "5  csv_max_income   people_large      25   30.263  37.220   \n",
       "6        csv_anna   people_small      25    0.060   0.085   \n",
       "7        csv_anna  people_medium      25    0.688   0.740   \n",
       "8        csv_anna   people_large      25   34.957  39.114   \n",
       "\n",
       "   maximum                                          \n",
       "0                                            2.282  \n",
       "1                                            2.892  \n",
       "2                                           32.163  \n",
       "3                                            1.884  \n",
       "4                                            5.779  \n",
       "5                                           42.168  \n",
       "6                                            0.218  \n",
       "7                                            1.246  \n",
       "8                                           43.670  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = pd.read_clipboard(sep=\",\")\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CSV vs Parquet\n",
    "\n",
    "There are two steps here. First copy some files. Second copy and paste your SQL queries.\n",
    "\n",
    "### How do I copy files?\n",
    "\n",
    "Ok, this isn't quite as trivial as it sounds. We are actually converting the csv files from before into parquet files. This is going from semi-structured data to structured data. The easiest way is using pyspark (and the [copy helper function](https://github.com/sroy2/parquet_optimization/blob/master/bench.py#L77-L92) in bench.py).\n",
    "\n",
    "```python\n",
    "bench.copy(spark)\n",
    "```\n",
    "\n",
    "Yep, it's that easy. So is copying your queries - that just takes more pasting.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <strong>(Warning)</strong> Don't forget to change the spark.read.<strong>csv</strong> to spark.read.<strong>parquet</strong>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>(Hint)</strong> Don't forget your parquet files, we need them to move forward!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "bench.copy(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rinse and repeat.\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <strong>(Problem 2)</strong> Record the parquet results and compare to the csv results:\n",
    "</div>\n",
    "\n",
    "```python\n",
    ">>> test = \"bench.benchmark(spark, 25, queries.pq_anna, 'hdfs:/user/\"+bench.whoami()+\"/people_small.parquet')\"\n",
    ">>> bench.csv_table(spark, test, 'all pq')\n",
    "query,file,trials,minimum,median,maximum                                        \n",
    "pq_avg_income,people_small,25,1.925,2.578,3.366\n",
    "pq_avg_income,people_medium,25,2.392,3.216,5.029\n",
    "pq_avg_income,people_large,25,1.983,2.395,3.569\n",
    "pq_max_income,people_small,25,1.969,2.374,3.312\n",
    "pq_max_income,people_medium,25,2.018,2.505,3.676\n",
    "pq_max_income,people_large,25,1.993,2.604,3.960\n",
    "pq_anna,people_small,25,0.076,0.081,0.197\n",
    "pq_anna,people_medium,25,0.137,0.153,0.197\n",
    "pq_anna,people_large,25,0.084,0.089,0.124\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <strong>(Warning)</strong> You could also run something like 'all both' if you want the both csv and pq results in one table. Keep in mind searching \"large\" csv files takes <strong>significantly more time</strong> than searching \"large\" pq files.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "test = \"bench.benchmark(spark, 25, queries.pq_anna, 'hdfs:/user/\"+bench.whoami()+\"/people_small.parquet')\"\n",
    "bench.csv_table(spark, test, 'all pq')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>file</th>\n",
       "      <th>trials</th>\n",
       "      <th>minimum</th>\n",
       "      <th>median</th>\n",
       "      <th>maximum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pq_avg_income</td>\n",
       "      <td>people_small</td>\n",
       "      <td>25</td>\n",
       "      <td>1.925</td>\n",
       "      <td>2.578</td>\n",
       "      <td>3.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pq_avg_income</td>\n",
       "      <td>people_medium</td>\n",
       "      <td>25</td>\n",
       "      <td>2.392</td>\n",
       "      <td>3.216</td>\n",
       "      <td>5.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pq_avg_income</td>\n",
       "      <td>people_large</td>\n",
       "      <td>25</td>\n",
       "      <td>1.983</td>\n",
       "      <td>2.395</td>\n",
       "      <td>3.569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pq_max_income</td>\n",
       "      <td>people_small</td>\n",
       "      <td>25</td>\n",
       "      <td>1.969</td>\n",
       "      <td>2.374</td>\n",
       "      <td>3.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pq_max_income</td>\n",
       "      <td>people_medium</td>\n",
       "      <td>25</td>\n",
       "      <td>2.018</td>\n",
       "      <td>2.505</td>\n",
       "      <td>3.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pq_max_income</td>\n",
       "      <td>people_large</td>\n",
       "      <td>25</td>\n",
       "      <td>1.993</td>\n",
       "      <td>2.604</td>\n",
       "      <td>3.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pq_anna</td>\n",
       "      <td>people_small</td>\n",
       "      <td>25</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pq_anna</td>\n",
       "      <td>people_medium</td>\n",
       "      <td>25</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pq_anna</td>\n",
       "      <td>people_large</td>\n",
       "      <td>25</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           query           file  trials  minimum  median  \\\n",
       "0  pq_avg_income   people_small      25    1.925   2.578   \n",
       "1  pq_avg_income  people_medium      25    2.392   3.216   \n",
       "2  pq_avg_income   people_large      25    1.983   2.395   \n",
       "3  pq_max_income   people_small      25    1.969   2.374   \n",
       "4  pq_max_income  people_medium      25    2.018   2.505   \n",
       "5  pq_max_income   people_large      25    1.993   2.604   \n",
       "6        pq_anna   people_small      25    0.076   0.081   \n",
       "7        pq_anna  people_medium      25    0.137   0.153   \n",
       "8        pq_anna   people_large      25    0.084   0.089   \n",
       "\n",
       "   maximum                                          \n",
       "0                                            3.366  \n",
       "1                                            5.029  \n",
       "2                                            3.569  \n",
       "3                                            3.312  \n",
       "4                                            3.676  \n",
       "5                                            3.960  \n",
       "6                                            0.197  \n",
       "7                                            0.197  \n",
       "8                                            0.124  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3 = pd.read_clipboard(sep=\",\")\n",
    "df_3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Optimizing Parquet\n",
    "\n",
    "Ok. The part you were waiting for!\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  <strong>(Problem 3)</strong> Try at least three different ways of optimizing parquet files and search for the best configurations. Record the optimized results and compare to the base results above.\n",
    "</div>\n",
    "\n",
    "1. This is not a comprehensive parquet optimization guide... just enough to think about how to get through this homework.\n",
    "2. If you don't care about learning the stuff... just look for the green blocks.\n",
    "3. You still need to do a write up at the end!\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <strong>(Warning)</strong> You aren't allowed to change any of the query code that you wrote in the previous step. So don't try it!\n",
    "</div>\n",
    "\n",
    "So how do we speed queries up?\n",
    "\n",
    "<img src=\"img/10.png\"/>\n",
    "\n",
    "## Sorting\n",
    "\n",
    "Sorting is useful because it helps organize your columns before they get broken up into row groups. The footer information at the bottom of each page gives basic statistics (like min or max values) that can eliminate entire pages from being read. \n",
    "\n",
    "```python\n",
    "df = spark.read.parquet(...)\n",
    "\n",
    "#Sort Examples:\n",
    "df.sort('zipcode')\n",
    "df.sort('zipcode', 'income')\n",
    "```\n",
    "\n",
    "But wait you ask - if it's still in one big parquet file, how does that make it any faster? Well... by itself, it might not.\n",
    "\n",
    "*But what do I sort?*\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    You should probably have an idea of what to sort just by reading your query. When it isn't obvious you can call the .explain() on your dataframe. Note - the physical plan from .explain() is read from the bottom up.\n",
    "</div>\n",
    "\n",
    "```python\n",
    ">>> file = \"hdfs:/user/\"+bench.whoami()+\"/people_small.parquet\"\n",
    ">>> queries.pq_anna(spark, file).explain()\n",
    "== Physical Plan ==\n",
    "*(1) Project [first_name#8537, last_name#8538, income#8539]\n",
    "+- *(1) Filter (((isnotnull(first_name#8537) && isnotnull(income#8539)) && (first_name#8537 = Anna)) && (income#8539 >= 70000.0))\n",
    "   +- *(1) FileScan parquet [first_name#8537,last_name#8538,income#8539] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://dumbo/user/sr5388/people_small.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(first_name), IsNotNull(income), EqualTo(first_name,Anna), GreaterThanOrEqual(income,70..., ReadSchema: struct<first_name:string,last_name:string,income:float>\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>(Hint)</strong> Here we see three different columns being used in FileScan. To optimize most effectively, we therefore want to sort all three columns. This helps maximize the number of row groups we can skip! Additionally, sorted data usually compresses much better, thus taking less space/time to read. <strong>(Warning)</strong> Sorting has an up-front cost that and may not be useful in all applications. In some cases it may result in excess partitions/tiny files, negatively impacting read times.\n",
    "</div>\n",
    "\n",
    "*But... you implemented something for us to just automate everything right?*\n",
    "\n",
    "1. Haha! Not this... I mean... FINE.\n",
    "2. There are really just two steps: choosing transformations and writing parquet files\n",
    "3. The transform helper function in bench.py is your friend, transformations are applied **IN THE ORDER** they appear in the list! This can make a difference.\n",
    "\n",
    "```python\n",
    "#From bench.transform(spark, t_list, name, benchmark_cmd=None, runtype=False)\n",
    "    t_list : list of tuples\n",
    "        list of transformations to apply:\n",
    "        ('filename','sort','value')\n",
    "        ('filename','repartition','value')\n",
    "        \n",
    "        where filenames = 'people_small'|0, 'people_medium'|1, 'people_large'|2\n",
    "        \n",
    "    name : string\n",
    "        string appended to base filename when adding to hdfs\n",
    "        'people_small' -> 'people_small'+'_'+name\n",
    "```\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <strong>(Hint)</strong> I recommend the following workflow:\n",
    "</div>\n",
    "\n",
    "1. Start with ONE query.\n",
    "2. Decide what you want to tweak for that query.\n",
    "3. Build the transformation list, you can address different files with different strategies.\n",
    "4. Run the transform.\n",
    "5. Benchmark the new parquet files.\n",
    "\n",
    "```python\n",
    ">>> t_list = [('people_small', 'sort', 'zipcode'),\n",
    "              (1, 'sort', 'zipcode'),\n",
    "              (2, 'sort', 'zipcode, income, first_name')]\n",
    ">>> test = \"bench.benchmark(spark, 25, queries.pq_anna, 'hdfs:/user/\"+bench.whoami()+\"/people_small_sort.parquet')\"\n",
    ">>> bench.transform(spark, t_list, 'q1_sort')\n",
    ">>> bench.csv_table(spark, test, 'pq')\n",
    "query,file,trials,minimum,median,maximum                                        \n",
    "pq_anna,people_small,25,0.068,0.078,0.195\n",
    "pq_anna,people_medium,25,1.133,1.412,3.168\n",
    "pq_anna,people_large,25,1.081,1.607,2.743\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    You can automatically chain together transform and csv_table by providing the benchmark_cmd to transform directly: <br>\n",
    "    bench.transform(spark=spark, t_list=t_list, name='q1_sort', benchmark_cmd=test, runtype='pq')\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "t_list = [('people_small', 'sort', 'zipcode'),\n",
    "          (1, 'sort', 'zipcode'),\n",
    "          (2, 'sort', 'zipcode, income, first_name')]\n",
    "test = \"bench.benchmark(spark, 25, queries.pq_anna, 'hdfs:/user/\"+bench.whoami()+\"/people_small_q1_sort.parquet')\"\n",
    "\n",
    "bench.transform(spark, t_list, 'q1_sort')\n",
    "bench.csv_table(spark, test, 'pq')\n",
    "\n",
    "#bench.transform(spark, t_list, 'q1_sort', test, 'pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Partitioning\n",
    "\n",
    "### Method 1 - (slow)\n",
    "\n",
    "Explicitly setting the partition columns when writing out to parquet is a slower, but arguably more robust method of partitioning. I didn't include this as a method you can call in transform... call me lazy. This will make an individual file for every unique value in the column you partitionBy. Recall having many small files is often poor for optimization. To test it out, try something akin to the following:\n",
    "\n",
    "```python\n",
    ">>> file = 'hdfs:/user/'+bench.whoami()+'/'+x\n",
    ">>> df = spark.read.parquet(file+'.parquet')\n",
    ">>> df.write.partitionBy(\"zipcode\").parquet(file+'_partition.parquet')\n",
    "```\n",
    "\n",
    "\n",
    "### Method 2 - (fast)\n",
    "\n",
    "We are just going to do the same thing we did with sorting above.\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 - THIS IS SLOW!!!\n",
    "for x in ['people_small', 'people_medium', 'people_large']:\n",
    "    file = 'hdfs:/user/'+bench.whoami()+'/'+x\n",
    "    df = spark.read.parquet(file+'.parquet')\n",
    "    df.write.partitionBy(\"zipcode\").parquet(file+'_partition.parquet')\n",
    "    \n",
    "# Method 2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
